{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import LSTM,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851259</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZBH</td>\n",
       "      <td>103.309998</td>\n",
       "      <td>103.199997</td>\n",
       "      <td>102.849998</td>\n",
       "      <td>103.930000</td>\n",
       "      <td>973800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851260</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>43.070000</td>\n",
       "      <td>43.040001</td>\n",
       "      <td>42.689999</td>\n",
       "      <td>43.310001</td>\n",
       "      <td>1938100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851261</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>53.639999</td>\n",
       "      <td>53.529999</td>\n",
       "      <td>53.270000</td>\n",
       "      <td>53.740002</td>\n",
       "      <td>1701200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851262</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>AIV</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>45.450001</td>\n",
       "      <td>44.410000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>1380900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851263</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>FTV</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>53.630001</td>\n",
       "      <td>53.389999</td>\n",
       "      <td>54.480000</td>\n",
       "      <td>705100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>851264 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date symbol        open       close         low        high  \\\n",
       "0       2016-01-05   WLTW  123.430000  125.839996  122.309998  126.250000   \n",
       "1       2016-01-06   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "2       2016-01-07   WLTW  116.379997  114.949997  114.930000  119.739998   \n",
       "3       2016-01-08   WLTW  115.480003  116.620003  113.500000  117.440002   \n",
       "4       2016-01-11   WLTW  117.010002  114.970001  114.089996  117.330002   \n",
       "...            ...    ...         ...         ...         ...         ...   \n",
       "851259  2016-12-30    ZBH  103.309998  103.199997  102.849998  103.930000   \n",
       "851260  2016-12-30   ZION   43.070000   43.040001   42.689999   43.310001   \n",
       "851261  2016-12-30    ZTS   53.639999   53.529999   53.270000   53.740002   \n",
       "851262  2016-12-30    AIV   44.730000   45.450001   44.410000   45.590000   \n",
       "851263  2016-12-30    FTV   54.200001   53.630001   53.389999   54.480000   \n",
       "\n",
       "           volume  \n",
       "0       2163600.0  \n",
       "1       2386400.0  \n",
       "2       2489500.0  \n",
       "3       2006300.0  \n",
       "4       1408600.0  \n",
       "...           ...  \n",
       "851259   973800.0  \n",
       "851260  1938100.0  \n",
       "851261  1701200.0  \n",
       "851262  1380900.0  \n",
       "851263   705100.0  \n",
       "\n",
       "[851264 rows x 7 columns]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_securities = pd.read_csv('./data/securities.csv')\n",
    "df_prices = pd.read_csv('./data/prices-split-adjusted.csv')\n",
    "df_prices.fillna(float(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companies(num):\n",
    "    choices = []\n",
    "    names = []\n",
    "    while(len(choices) < 5):\n",
    "        i = random.randint(0,df_securities.shape[0]-1)\n",
    "        symbol = df_securities['Ticker symbol'].iloc[i]\n",
    "        name = df_securities['Security'].iloc[i]\n",
    "        temp_df = df_prices[df_prices['symbol']==symbol]\n",
    "        if (temp_df.shape[0] > 1500) and (symbol not in choices):\n",
    "            choices.append(symbol)\n",
    "            names.append(name)\n",
    "        \n",
    "    return choices, names\n",
    "\n",
    "\n",
    "def get_multiple_df(symbols):\n",
    "    directory = []\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        single_df = df_prices[df_prices['symbol']==symbol]\n",
    "        single_df = single_df.drop(columns=['symbol'])\n",
    "        single_df = single_df.set_index('date')\n",
    "        single_df = single_df['close'].to_frame()\n",
    "        \n",
    "        directory.append(single_df)\n",
    "    return directory\n",
    "\n",
    "\n",
    "def fix_data(data, scalar):\n",
    "    return scalar.inverse_transform(np.array(data).reshape(-1,1))\n",
    "\n",
    "\n",
    "def build_model(optimizer, step):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(LSTM(64,return_sequences=True,input_shape=(step,1)))\n",
    "    model.add(LSTM(50,return_sequences=True))\n",
    "    model.add(LSTM(32,return_sequences=True))\n",
    "    model.add(LSTM(50,return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer,loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def prep_forecast(prediction_list, scalar, split):\n",
    "    step = 10\n",
    "    \n",
    "    prediction_list = scalar.fit_transform(np.array(prediction_list).reshape(-1,1))\n",
    "    size = int(len(prediction_list)*split)\n",
    "    \n",
    "    forecast_data = prediction_list[0:size,:] #data\n",
    "    \n",
    "    x = []\n",
    "    for i in range(len(prediction_list)-step-1):\n",
    "        k = i+step\n",
    "        x.append(forecast_data[i:k,0])\n",
    "    x = np.array(x)\n",
    "    forecast_data = np.reshape(x,(x.shape[0],x.shape[1],1))\n",
    "    \n",
    "    return forecast_data\n",
    "\n",
    "\n",
    "class Company:\n",
    "    def __init__(self, symbol, name, df):\n",
    "        self.symbol = symbol\n",
    "        self.name = name\n",
    "        self.df = df\n",
    "        self.model = None\n",
    "        \n",
    "        self.train_data = []\n",
    "        self.test_data = []\n",
    "        \n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        self.y_hat = None\n",
    "        self.error = None\n",
    "        \n",
    "        self.history = None\n",
    "        self.forcast = None\n",
    "        \n",
    "        \n",
    "    def prep_data(self, scalar, split):\n",
    "        scaled_df = scalar.fit_transform(np.array(self.df).reshape(-1,1))\n",
    "        size = int(len(self.df)*split)\n",
    "\n",
    "        self.train_data = scaled_df[0:size,:]\n",
    "        self.test_data = scaled_df[size:,:]\n",
    "        \n",
    "    \n",
    "    def build_data(self, step, history_div):\n",
    "        x, y = [], []\n",
    "        for i in range(int(len(self.train_data)-step-1)):\n",
    "            k = i+step\n",
    "            x.append(self.train_data[i:k,0])\n",
    "            y.append(self.train_data[k,0])\n",
    "            \n",
    "        X_train = np.array(x)\n",
    "        X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))  \n",
    "        self.X_train = X_train\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "        x, y = [], []\n",
    "        for i in range(int(len(self.test_data)-step-1)):\n",
    "            k = i+step\n",
    "            x.append(self.test_data[i:k,0])\n",
    "            y.append(self.test_data[k,0])\n",
    "            \n",
    "        X_test = np.array(x)\n",
    "        X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1)      \n",
    "        self.X_test = X_test\n",
    "        self.y_test = np.array(y)\n",
    "        self.history = int(len(self.y_test)/history_div)\n",
    "        \n",
    "        \n",
    "    def train_model(self, step, scalar, epochs, batch):\n",
    "        self.model = build_model('adam',step)\n",
    "        self.model.fit(self.X_train,self.y_train,epochs=epochs,batch_size=batch)\n",
    "\n",
    "        self.y_hat = self.model.predict(self.X_test)\n",
    "        self.error = np.sqrt(mean_squared_error(self.y_test,self.y_hat))\n",
    "\n",
    "        self.y_test = fix_data(self.y_test, scalar)\n",
    "        self.y_hat = fix_data(self.y_hat, scalar)\n",
    "        print(self.error)\n",
    "        \n",
    "        \n",
    "    def get_forecast(self, scalar):\n",
    "        prediction_list = self.y_test[-self.history:]\n",
    "        forecast_data = prep_forecast(prediction_list, scalar, 1)\n",
    "        \n",
    "        forecast_data = self.model.predict(forecast_data)\n",
    "        forecast_data = fix_data(forecast_data, scalar)\n",
    "        self.forecast = forecast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EW: Edwards Lifesciences\n",
      "X Training Shape: (1172, 60, 1)\n",
      "X Test Shape: (468, 60, 1)\n",
      "Y Training Shape: (1172,)\n",
      "Y Test Shape: (468,)\n",
      "Epoch 1/60\n",
      "118/118 [==============================] - 13s 113ms/step - loss: 0.0024\n",
      "Epoch 2/60\n",
      "118/118 [==============================] - 14s 117ms/step - loss: 7.2219e-04\n",
      "Epoch 3/60\n",
      "118/118 [==============================] - 14s 120ms/step - loss: 6.7245e-04\n",
      "Epoch 4/60\n",
      "118/118 [==============================] - 15s 124ms/step - loss: 6.8762e-04\n",
      "Epoch 5/60\n",
      "118/118 [==============================] - 13s 114ms/step - loss: 4.7183e-04\n",
      "Epoch 6/60\n",
      "118/118 [==============================] - 15s 125ms/step - loss: 4.3673e-04\n",
      "Epoch 7/60\n",
      "118/118 [==============================] - 13s 110ms/step - loss: 3.5460e-04\n",
      "Epoch 8/60\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 2.8961e-04\n",
      "Epoch 9/60\n",
      "118/118 [==============================] - 13s 113ms/step - loss: 2.5227e-04\n",
      "Epoch 10/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 2.6147e-04\n",
      "Epoch 11/60\n",
      "118/118 [==============================] - 14s 117ms/step - loss: 1.9509e-04\n",
      "Epoch 12/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 2.0157e-04\n",
      "Epoch 13/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 1.6931e-04\n",
      "Epoch 14/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 1.4962e-04\n",
      "Epoch 15/60\n",
      "118/118 [==============================] - 13s 113ms/step - loss: 1.3987e-04\n",
      "Epoch 16/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 1.4875e-04\n",
      "Epoch 17/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 1.3264e-04\n",
      "Epoch 18/60\n",
      "118/118 [==============================] - 13s 110ms/step - loss: 1.4455e-04\n",
      "Epoch 19/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 1.1970e-04\n",
      "Epoch 20/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 1.0928e-04\n",
      "Epoch 21/60\n",
      "118/118 [==============================] - 14s 117ms/step - loss: 1.0581e-04\n",
      "Epoch 22/60\n",
      "118/118 [==============================] - 13s 109ms/step - loss: 1.2606e-04\n",
      "Epoch 23/60\n",
      "118/118 [==============================] - 13s 107ms/step - loss: 1.1586e-04\n",
      "Epoch 24/60\n",
      "118/118 [==============================] - 13s 107ms/step - loss: 9.4082e-05\n",
      "Epoch 25/60\n",
      "118/118 [==============================] - 13s 107ms/step - loss: 1.0506e-04\n",
      "Epoch 26/60\n",
      "118/118 [==============================] - 13s 107ms/step - loss: 9.6412e-05\n",
      "Epoch 27/60\n",
      "118/118 [==============================] - 13s 108ms/step - loss: 9.3474e-05\n",
      "Epoch 28/60\n",
      "118/118 [==============================] - 13s 107ms/step - loss: 9.4666e-05\n",
      "Epoch 29/60\n",
      "118/118 [==============================] - 13s 108ms/step - loss: 1.0195e-04\n",
      "Epoch 30/60\n",
      "118/118 [==============================] - 13s 110ms/step - loss: 1.0373e-04\n",
      "Epoch 31/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 1.0408e-04\n",
      "Epoch 32/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 1.0118e-04\n",
      "Epoch 33/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 1.0453e-04\n",
      "Epoch 34/60\n",
      "118/118 [==============================] - 13s 108ms/step - loss: 8.8415e-05\n",
      "Epoch 35/60\n",
      "118/118 [==============================] - 13s 109ms/step - loss: 9.1985e-05\n",
      "Epoch 36/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 1.1603e-04\n",
      "Epoch 37/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 9.4751e-05\n",
      "Epoch 38/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 9.3405e-05\n",
      "Epoch 39/60\n",
      "118/118 [==============================] - 14s 117ms/step - loss: 9.2640e-05\n",
      "Epoch 40/60\n",
      "118/118 [==============================] - 14s 116ms/step - loss: 9.0736e-05\n",
      "Epoch 41/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 8.7626e-05\n",
      "Epoch 42/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 1.1191e-04\n",
      "Epoch 43/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 1.0539e-04\n",
      "Epoch 44/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 9.7860e-05\n",
      "Epoch 45/60\n",
      "118/118 [==============================] - 13s 110ms/step - loss: 9.2547e-05\n",
      "Epoch 46/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 9.6622e-05\n",
      "Epoch 47/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 8.5045e-05\n",
      "Epoch 48/60\n",
      "118/118 [==============================] - 14s 117ms/step - loss: 8.8772e-05\n",
      "Epoch 49/60\n",
      "118/118 [==============================] - 14s 116ms/step - loss: 9.2821e-05\n",
      "Epoch 50/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 9.1466e-051s - los\n",
      "Epoch 51/60\n",
      "118/118 [==============================] - 13s 109ms/step - loss: 9.0057e-05\n",
      "Epoch 52/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 1.3053e-04\n",
      "Epoch 53/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 8.9977e-05\n",
      "Epoch 54/60\n",
      "118/118 [==============================] - 13s 112ms/step - loss: 8.6796e-05\n",
      "Epoch 55/60\n",
      "118/118 [==============================] - 13s 113ms/step - loss: 9.7870e-05\n",
      "Epoch 56/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 9.2153e-05\n",
      "Epoch 57/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 1.0464e-04\n",
      "Epoch 58/60\n",
      "118/118 [==============================] - 14s 115ms/step - loss: 8.5584e-05\n",
      "Epoch 59/60\n",
      "118/118 [==============================] - 13s 110ms/step - loss: 8.5423e-05\n",
      "Epoch 60/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 9.2974e-05\n",
      "0.04861362673292939\n",
      "============================================================\n",
      "ANTM: Anthem Inc.\n",
      "X Training Shape: (1172, 60, 1)\n",
      "X Test Shape: (468, 60, 1)\n",
      "Y Training Shape: (1172,)\n",
      "Y Test Shape: (468,)\n",
      "Epoch 1/60\n",
      "118/118 [==============================] - 13s 114ms/step - loss: 0.0050\n",
      "Epoch 2/60\n",
      "118/118 [==============================] - 13s 110ms/step - loss: 0.0011\n",
      "Epoch 3/60\n",
      "118/118 [==============================] - 13s 109ms/step - loss: 8.8585e-04\n",
      "Epoch 4/60\n",
      "118/118 [==============================] - 14s 116ms/step - loss: 8.2974e-04\n",
      "Epoch 5/60\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 7.6609e-04\n",
      "Epoch 6/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 6.0753e-04\n",
      "Epoch 7/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 4.8971e-04\n",
      "Epoch 8/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 5.0237e-04\n",
      "Epoch 9/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 4.7918e-04\n",
      "Epoch 10/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 3.9299e-04\n",
      "Epoch 11/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 5.3463e-04\n",
      "Epoch 12/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 4.4452e-04\n",
      "Epoch 13/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 3.6517e-04\n",
      "Epoch 14/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 3.1533e-04\n",
      "Epoch 15/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 2.8170e-04\n",
      "Epoch 16/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 2.6567e-04\n",
      "Epoch 17/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 2.4790e-04\n",
      "Epoch 18/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 2.2676e-04\n",
      "Epoch 19/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 2.1495e-04\n",
      "Epoch 20/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 2.1934e-04\n",
      "Epoch 21/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.8134e-04\n",
      "Epoch 22/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.8559e-04\n",
      "Epoch 23/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.7317e-04\n",
      "Epoch 24/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.5280e-04\n",
      "Epoch 25/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.5241e-04\n",
      "Epoch 26/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.5516e-04\n",
      "Epoch 27/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.6753e-04\n",
      "Epoch 28/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 12s 103ms/step - loss: 1.3159e-04\n",
      "Epoch 29/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.3155e-04\n",
      "Epoch 30/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.4509e-04\n",
      "Epoch 31/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.3664e-04\n",
      "Epoch 32/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.2891e-04\n",
      "Epoch 33/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.5707e-04\n",
      "Epoch 34/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.5070e-04\n",
      "Epoch 35/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.3656e-04\n",
      "Epoch 36/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.5287e-04\n",
      "Epoch 37/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.4145e-04\n",
      "Epoch 38/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.3802e-04\n",
      "Epoch 39/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.2481e-04\n",
      "Epoch 40/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.5323e-041s - lo\n",
      "Epoch 41/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.4272e-04\n",
      "Epoch 42/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.3134e-04\n",
      "Epoch 43/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.1624e-04\n",
      "Epoch 44/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.0807e-04\n",
      "Epoch 45/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.1998e-04\n",
      "Epoch 46/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.3804e-04\n",
      "Epoch 47/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.2225e-04\n",
      "Epoch 48/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.3873e-04\n",
      "Epoch 49/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.3547e-04\n",
      "Epoch 50/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.2384e-04\n",
      "Epoch 51/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.2249e-04\n",
      "Epoch 52/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.2366e-04\n",
      "Epoch 53/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.6995e-04\n",
      "Epoch 54/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.3630e-04\n",
      "Epoch 55/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.1476e-04\n",
      "Epoch 56/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.2277e-04\n",
      "Epoch 57/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.1681e-04\n",
      "Epoch 58/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 1.0468e-04\n",
      "Epoch 59/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.1867e-04\n",
      "Epoch 60/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 1.4588e-04\n",
      "0.02186515817252524\n",
      "============================================================\n",
      "BK: The Bank of New York Mellon Corp.\n",
      "X Training Shape: (1172, 60, 1)\n",
      "X Test Shape: (468, 60, 1)\n",
      "Y Training Shape: (1172,)\n",
      "Y Test Shape: (468,)\n",
      "Epoch 1/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 0.0082\n",
      "Epoch 2/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 0.0026\n",
      "Epoch 3/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 0.0019\n",
      "Epoch 4/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 0.0014\n",
      "Epoch 5/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 0.0012\n",
      "Epoch 6/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 9.5476e-04\n",
      "Epoch 7/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 9.6646e-04\n",
      "Epoch 8/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 9.0103e-04\n",
      "Epoch 9/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 7.9930e-04\n",
      "Epoch 10/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 7.0527e-04\n",
      "Epoch 11/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 6.2051e-04\n",
      "Epoch 12/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 5.4994e-04\n",
      "Epoch 13/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 5.6681e-04\n",
      "Epoch 14/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 5.0298e-04\n",
      "Epoch 15/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 4.7200e-04\n",
      "Epoch 16/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 4.9828e-04\n",
      "Epoch 17/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 3.9544e-04\n",
      "Epoch 18/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 4.9846e-04\n",
      "Epoch 19/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 3.8117e-04\n",
      "Epoch 20/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 4.4387e-040s - loss: 4.4387e-0\n",
      "Epoch 21/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 3.2984e-04\n",
      "Epoch 22/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 2.9095e-04\n",
      "Epoch 23/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 3.0444e-04\n",
      "Epoch 24/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 3.1165e-04\n",
      "Epoch 25/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 2.7239e-04\n",
      "Epoch 26/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 2.7395e-040s - loss: 2.\n",
      "Epoch 27/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 3.1440e-04\n",
      "Epoch 28/60\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 2.8858e-04\n",
      "Epoch 29/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 2.6304e-04\n",
      "Epoch 30/60\n",
      "118/118 [==============================] - 13s 107ms/step - loss: 2.4156e-04\n",
      "Epoch 31/60\n",
      "118/118 [==============================] - 14s 115ms/step - loss: 2.8147e-04\n",
      "Epoch 32/60\n",
      "118/118 [==============================] - 13s 109ms/step - loss: 2.4286e-04\n",
      "Epoch 33/60\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 2.8562e-04\n",
      "Epoch 34/60\n",
      "118/118 [==============================] - 14s 118ms/step - loss: 2.9532e-04\n",
      "Epoch 35/60\n",
      "118/118 [==============================] - 14s 116ms/step - loss: 3.2752e-04\n",
      "Epoch 36/60\n",
      "118/118 [==============================] - 14s 117ms/step - loss: 2.8354e-04\n",
      "Epoch 37/60\n",
      "118/118 [==============================] - 13s 109ms/step - loss: 2.5263e-04\n",
      "Epoch 38/60\n",
      "118/118 [==============================] - 12s 100ms/step - loss: 2.4687e-04\n",
      "Epoch 39/60\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 2.9151e-04\n",
      "Epoch 40/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 2.9630e-04\n",
      "Epoch 41/60\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 2.4513e-04\n",
      "Epoch 42/60\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 2.3904e-04\n",
      "Epoch 43/60\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 2.5874e-04\n",
      "Epoch 44/60\n",
      "118/118 [==============================] - 12s 100ms/step - loss: 2.5216e-04\n",
      "Epoch 45/60\n",
      "118/118 [==============================] - 12s 100ms/step - loss: 2.2120e-04\n",
      "Epoch 46/60\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 2.6476e-04\n",
      "Epoch 47/60\n",
      "118/118 [==============================] - 12s 100ms/step - loss: 2.7002e-04\n",
      "Epoch 48/60\n",
      "118/118 [==============================] - 12s 100ms/step - loss: 2.4959e-04\n",
      "Epoch 49/60\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 2.1188e-04\n",
      "Epoch 50/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 2.3256e-04\n",
      "Epoch 51/60\n",
      "118/118 [==============================] - 13s 108ms/step - loss: 2.5361e-04\n",
      "Epoch 52/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 2.6317e-04\n",
      "Epoch 53/60\n",
      "118/118 [==============================] - 12s 100ms/step - loss: 2.3218e-04\n",
      "Epoch 54/60\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 2.1074e-04 5s - l - ETA: 3s - loss: 2 \n",
      "Epoch 55/60\n",
      "118/118 [==============================] - 12s 100ms/step - loss: 2.4204e-04\n",
      "Epoch 56/60\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 2.5694e-04\n",
      "Epoch 57/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 12s 100ms/step - loss: 2.4652e-04\n",
      "Epoch 58/60\n",
      "118/118 [==============================] - 12s 100ms/step - loss: 2.5110e-044s -\n",
      "Epoch 59/60\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 2.2277e-04\n",
      "Epoch 60/60\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 2.7426e-04\n",
      "0.03976502522475036\n",
      "============================================================\n",
      "HOLX: Hologic\n",
      "X Training Shape: (1172, 60, 1)\n",
      "X Test Shape: (468, 60, 1)\n",
      "Y Training Shape: (1172,)\n",
      "Y Test Shape: (468,)\n",
      "Epoch 1/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 0.0033\n",
      "Epoch 2/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 0.0015\n",
      "Epoch 3/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 0.0011\n",
      "Epoch 4/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 8.6710e-04\n",
      "Epoch 5/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 7.3790e-04\n",
      "Epoch 6/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 5.7402e-04\n",
      "Epoch 7/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 5.0630e-04\n",
      "Epoch 8/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 4.7182e-04\n",
      "Epoch 9/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 4.5867e-04\n",
      "Epoch 10/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 3.8247e-043s - loss: 4.0\n",
      "Epoch 11/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 3.3949e-04\n",
      "Epoch 12/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 4.2131e-04\n",
      "Epoch 13/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 2.9059e-04\n",
      "Epoch 14/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 2.5967e-04\n",
      "Epoch 15/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 2.7506e-04\n",
      "Epoch 16/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 2.4344e-04\n",
      "Epoch 17/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 2.3962e-04\n",
      "Epoch 18/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.9549e-04\n",
      "Epoch 19/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.8740e-04\n",
      "Epoch 20/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 2.0030e-04\n",
      "Epoch 21/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 2.1040e-04\n",
      "Epoch 22/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.8540e-04\n",
      "Epoch 23/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.8316e-04\n",
      "Epoch 24/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 2.1286e-04\n",
      "Epoch 25/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 2.0827e-04\n",
      "Epoch 26/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 2.1788e-04\n",
      "Epoch 27/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 1.6958e-04\n",
      "Epoch 28/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 2.1286e-04\n",
      "Epoch 29/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 1.7930e-04\n",
      "Epoch 30/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.9138e-04\n",
      "Epoch 31/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 2.0520e-04\n",
      "Epoch 32/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 1.7950e-04\n",
      "Epoch 33/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.8745e-04\n",
      "Epoch 34/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 2.0295e-04\n",
      "Epoch 35/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7100e-04\n",
      "Epoch 36/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 1.8318e-04\n",
      "Epoch 37/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7474e-04\n",
      "Epoch 38/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7245e-04\n",
      "Epoch 39/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.6692e-04\n",
      "Epoch 40/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.9873e-04\n",
      "Epoch 41/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 1.7055e-04\n",
      "Epoch 42/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.8781e-04\n",
      "Epoch 43/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7460e-04\n",
      "Epoch 44/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7271e-04\n",
      "Epoch 45/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 1.8318e-04\n",
      "Epoch 46/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 1.7211e-04\n",
      "Epoch 47/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7511e-04\n",
      "Epoch 48/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 1.5695e-04\n",
      "Epoch 49/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7718e-04\n",
      "Epoch 50/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.6719e-04\n",
      "Epoch 51/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7699e-04\n",
      "Epoch 52/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 1.6771e-04\n",
      "Epoch 53/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.8592e-04\n",
      "Epoch 54/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.8180e-04\n",
      "Epoch 55/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7630e-04\n",
      "Epoch 56/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7078e-04\n",
      "Epoch 57/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.7812e-04\n",
      "Epoch 58/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.6432e-04\n",
      "Epoch 59/60\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 1.5449e-04\n",
      "Epoch 60/60\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 1.9004e-04\n",
      "0.09541779833539261\n",
      "============================================================\n",
      "UHS: Universal Health Services, Inc.\n",
      "X Training Shape: (1172, 60, 1)\n",
      "X Test Shape: (468, 60, 1)\n",
      "Y Training Shape: (1172,)\n",
      "Y Test Shape: (468,)\n",
      "Epoch 1/60\n",
      "118/118 [==============================] - 13s 108ms/step - loss: 0.0033\n",
      "Epoch 2/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 0.0011\n",
      "Epoch 3/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 9.2585e-04\n",
      "Epoch 4/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 0.0011\n",
      "Epoch 5/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 7.3836e-04\n",
      "Epoch 6/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 6.7929e-04\n",
      "Epoch 7/60\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 6.2591e-04\n",
      "Epoch 8/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 6.0521e-04\n",
      "Epoch 9/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 5.4229e-04\n",
      "Epoch 10/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 5.0263e-04\n",
      "Epoch 11/60\n",
      "118/118 [==============================] - 13s 107ms/step - loss: 4.8369e-04\n",
      "Epoch 12/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 4.2106e-04\n",
      "Epoch 13/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 3.4682e-04\n",
      "Epoch 14/60\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 3.1198e-04\n",
      "Epoch 15/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 3.4983e-04\n",
      "Epoch 16/60\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 2.7956e-04\n",
      "Epoch 17/60\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 2.7112e-04\n",
      "Epoch 18/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 2.6560e-04\n",
      "Epoch 19/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 2.2985e-04\n",
      "Epoch 20/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.8941e-04\n",
      "Epoch 21/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 2.0212e-04\n",
      "Epoch 22/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.8163e-04\n",
      "Epoch 23/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.9131e-040s - loss: 1.9131e-0\n",
      "Epoch 24/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 12s 105ms/step - loss: 3.3737e-04\n",
      "Epoch 25/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.2789e-04\n",
      "Epoch 26/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.4293e-04\n",
      "Epoch 27/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.3599e-04\n",
      "Epoch 28/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.2669e-041s - los\n",
      "Epoch 29/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.2252e-04\n",
      "Epoch 30/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.3760e-04\n",
      "Epoch 31/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.9435e-04\n",
      "Epoch 32/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.0953e-04\n",
      "Epoch 33/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.3888e-04\n",
      "Epoch 34/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.3197e-04\n",
      "Epoch 35/60\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 1.3629e-04\n",
      "Epoch 36/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.7263e-04\n",
      "Epoch 37/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.3113e-04\n",
      "Epoch 38/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.1340e-04\n",
      "Epoch 39/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.2156e-04\n",
      "Epoch 40/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.6787e-04\n",
      "Epoch 41/60\n",
      "118/118 [==============================] - 13s 107ms/step - loss: 1.3607e-04\n",
      "Epoch 42/60\n",
      "118/118 [==============================] - 13s 108ms/step - loss: 1.5732e-04\n",
      "Epoch 43/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.6807e-04\n",
      "Epoch 44/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.4302e-04\n",
      "Epoch 45/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.0321e-04\n",
      "Epoch 46/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.2115e-04\n",
      "Epoch 47/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.0234e-04\n",
      "Epoch 48/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.6060e-04\n",
      "Epoch 49/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.1565e-04\n",
      "Epoch 50/60\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 1.2511e-04\n",
      "Epoch 51/60\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 1.0263e-04\n",
      "Epoch 52/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.1055e-04\n",
      "Epoch 53/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.6279e-04\n",
      "Epoch 54/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.0949e-04\n",
      "Epoch 55/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.3530e-04\n",
      "Epoch 56/60\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 1.2637e-04\n",
      "Epoch 57/60\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 1.1420e-04\n",
      "Epoch 58/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.5439e-04\n",
      "Epoch 59/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 9.8789e-05\n",
      "Epoch 60/60\n",
      "118/118 [==============================] - 12s 106ms/step - loss: 1.4085e-04\n",
      "0.022188632197981784\n"
     ]
    }
   ],
   "source": [
    "directory_size = 5\n",
    "directory = []\n",
    "\n",
    "split = 0.7\n",
    "step = 60\n",
    "epochs = 60\n",
    "batch = 10\n",
    "history_div = 23\n",
    "\n",
    "scalar = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "symbols, names = get_companies(directory_size)\n",
    "selected_dfs = get_multiple_df(symbols)\n",
    "for i in range(directory_size):\n",
    "    directory.append(Company(symbols[i], names[i], selected_dfs[i]))\n",
    "    \n",
    "for company in directory:\n",
    "    print(\"============================================================\")\n",
    "    \n",
    "    company.prep_data(scalar, split)\n",
    "    company.build_data(step, history_div)\n",
    "    \n",
    "    print(company.symbol+\": \"+company.name)\n",
    "    print(\"X Training Shape: \"+str(company.X_train.shape))\n",
    "    print(\"X Test Shape: \"+str(company.X_test.shape))\n",
    "    print(\"Y Training Shape: \"+str(company.y_train.shape))\n",
    "    print(\"Y Test Shape: \"+str(company.y_test.shape))\n",
    "    \n",
    "    company.train_model(step, scalar, epochs, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    company.train_model(step, scalar, epochs, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(directory_size,1, figsize = (20,directory_size*7.5))\n",
    "\n",
    "for i in range(directory_size):\n",
    "    if directory_size > 1:\n",
    "        company = directory[i]\n",
    "\n",
    "        ax[i].plot(company.y_test,label='Actual Price')\n",
    "        ax[i].plot(company.y_hat, c='r', label='Predicted Price')\n",
    "        ax[i].set_title(company.symbol+\": \"+company.name+' Stock Prediction')\n",
    "        ax[i].set_ylabel('Stock Price')\n",
    "        ax[i].set_xlabel('Time')\n",
    "        ax[i].tick_params(axis='x',\n",
    "                       which='both',\n",
    "                       bottom=False,\n",
    "                       top=False,\n",
    "                       labelbottom=False)\n",
    "        ax[i].legend()\n",
    "    else:\n",
    "        company = directory[i]\n",
    "\n",
    "        ax.plot(company.y_test,label='Actual Price')\n",
    "        ax.plot(company.y_hat, c='r', label='Predicted Price')\n",
    "        ax.set_title(company.symbol+\": \"+company.name+' Stock Prediction')\n",
    "        ax.set_ylabel('Stock Price')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.tick_params(axis='x',\n",
    "                       which='both',\n",
    "                       bottom=False,\n",
    "                       top=False,\n",
    "                       labelbottom=False)\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(directory_size,1, figsize = (20,directory_size*7.5))\n",
    "for i in range(directory_size):\n",
    "    company = directory[i]\n",
    "    company.get_forecast(scalar)\n",
    "    \n",
    "    full_forecast = np.concatenate((company.y_test,company.forecast))\n",
    "    \n",
    "    graph_df = pd.DataFrame(full_forecast)\n",
    "    \n",
    "    if directory_size > 1:\n",
    "        ax[i].plot(graph_df, label='Recorded Price History')\n",
    "        ax[i].plot(graph_df.loc[graph_df.index >= len(company.y_test)], c='r',label='Future Prediction Price')\n",
    "\n",
    "        ax[i].set_title('ExonMobil Corporation Stock Prediction')\n",
    "        ax[i].set_ylabel('Stock Price')\n",
    "        ax[i].set_xlabel('Time (2010-2016)')\n",
    "        ax[i].tick_params(axis='x',\n",
    "                       which='both',\n",
    "                       bottom=False,\n",
    "                       top=False,\n",
    "                       labelbottom=False)\n",
    "        ax[i].legend()\n",
    "    else:\n",
    "        ax.plot(graph_df, label='Recorded Price History')\n",
    "        ax.plot(graph_df.loc[graph_df.index >= len(company.y_test)], c='r',label='Future Prediction Price')\n",
    "\n",
    "        ax.set_title('ExonMobil Corporation Stock Prediction')\n",
    "        ax.set_ylabel('Stock Price')\n",
    "        ax.set_xlabel('Time (2010-2016)')\n",
    "        ax.tick_params(axis='x',\n",
    "                       which='both',\n",
    "                       bottom=False,\n",
    "                       top=False,\n",
    "                       labelbottom=False)\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
